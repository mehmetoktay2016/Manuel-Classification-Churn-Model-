{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#Loading of libraries\nimport pandas as pd\nimport numpy as np\nfrom lightgbm import LGBMClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom scipy.stats import shapiro\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nimport warnings","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hello to everyone,\n\nWith the work I did this week, I will do a classification project without building any models. So how will you do it? We haven't heard of such a thing! We will do it using only means, medians and quartiles. So how? Come on let's see :)"},{"metadata":{},"cell_type":"markdown","source":"**What would this value set be if a threshold was requested in Churn?**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Loading the data set\ndf = pd.read_csv(\"../input/churn-123/churn kopyas.csv\")\n\n#observation of the first 2 lines\ndf.head(2)\n\n#Percentage of churn class found in data\n(2037) / (7963 + 2037) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We see how many observations each class passes in the data.\ndf.Exited.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We dropped some variables from our dataset with the code snippet below. The reason for this is; The id and name variables do not have any positive effect on the model we will install."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Dropping unrelated variables\nneed_drops = [\"RowNumber\",  \"Surname\",\"CustomerId\"]\ndf.drop(need_drops, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Questioning of missing values. It does not have any missing value.\ndf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have done a normality test below. This is very important to us. Because it will affect the success of the model. This is the test we did; shows whether the variables (columns) are distributed normally. Interpreting the results of our transaction; H0 is rejected because the pvalue values ​​of all variables are less than 0.05 in the normality tests. So all variables are not distributed normally."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Test of normality\n\nfor i in ['CreditScore' , 'Age', 'Balance', 'EstimatedSalary']:\n    print(i)\n    t_test , p_value = shapiro(df[i])\n    print('Test İstatistiği = %.4f, p-değeri = %.4f' % (t_test, p_value))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the one_hot_encoder function below; It is the conversion of non-numeric variables in the data to numerical values ​​(0 and 1). This is the language the machine will understand."},{"metadata":{"trusted":true},"cell_type":"code","source":"def one_hot_encoder(dataframe, category_freq=10, nan_as_category=False):\n    categorical_cols = [col for col in dataframe.columns if len(dataframe[col].value_counts()) < category_freq\n                        and dataframe[col].dtypes == 'O']\n\n    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, dummy_na=nan_as_category, drop_first=True)\n\n    return dataframe\n\ndf = one_hot_encoder(df)\n\n#first two observations\ndf.head(2)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With histogram, the distortions in the data are visually examined."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Drawing histogram\ndef hist_for_nums(data, numeric_cols):\n    col_counter = 0\n    data = data.copy()\n    for col in numeric_cols:\n        data[col].hist(bins=20)\n        plt.xlabel(col)\n        plt.title(col)\n        plt.show()\n        col_counter += 1\n    print(col_counter, \"variables have been plotted\")\n\n\n#Drawing a histogram for numerical variables.\n\na = ['CreditScore' , 'Age', 'Balance', 'EstimatedSalary']\n\nhist_for_nums(df, a)  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With the violin graph, we analyzed the states of the variables in the data according to the target variable."},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in a:\n    sns.catplot(x =\"Exited\" , y= i ,data = df , kind=\"violin\")\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Calculating correlations\ndf.corr().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# MODELLEME"},{"metadata":{"trusted":true},"cell_type":"code","source":"#In order to prepare for the model, we distinguish between the target variable and our independent values.\nX = df.drop(['Exited'], axis=1)\ny = df[[\"Exited\"]]\n\n#Dividing the data set into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify = y , random_state=46)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LightGBM\n\n#Create a model\nlgbm = LGBMClassifier(random_state=12345)\ncross_val_score(lgbm, X, y, cv=10).mean()\n\n# model tuning\nlgbm_params = {\"learning_rate\": [0.01, 0.1],\n               \"n_estimators\": [500, 1000],\n               \"max_depth\": [3, 5]}\n\n#Calculation of GridSearchCV\ngs_cv = GridSearchCV(lgbm,\n                     lgbm_params,\n                     cv=5,\n                     n_jobs=-1,\n                     verbose=2).fit(X, y)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The graphic created below is very important to us. Because it shows the importance levels of the variables. On the other hand, we will select 3 variables with high importance and perform a completely manual classification process."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Building models with the best parameters\nlgbm_tuned = LGBMClassifier(**gs_cv.best_params_).fit(X_train, y_train)\ncross_val_score(lgbm_tuned, X_test, y_test, cv=10).mean()\n\n\nfeature_imp = pd.Series(lgbm_tuned.feature_importances_,\n                        index=X.columns).sort_values(ascending=False)\n\n#Graphical representation of variable significance levels\nsns.barplot(x=feature_imp, y=feature_imp.index)\nplt.xlabel('Değişken Önem Skorları')\nplt.ylabel('Değişkenler')\nplt.title(\"Değişken Önem Düzeyleri\")\nplt.show()\n\n#cv operations (crossing)\nkfold = KFold(n_splits=10, random_state=123456)\ncv_results = cross_val_score(LGBMClassifier(), X_train, y_train, cv=kfold, scoring=\"accuracy\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In the Lightgbm algorithm, we select the variables that are most important to us with FeatureImportance option. These variables are Age, Balance and EstimatedSalary."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Choosing high level of importance variables\ndf = df[[\"Age\",\"Balance\",\"EstimatedSalary\",\"Exited\"]]\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we might think that why do we create a threshold value?\n\nThis is because; It is the process of determining the limit to separate the classes of each variable and classifying them manually according to these values. Now the following may come to mind; Why did you use median instead of mean? This is because; We did the normality test at first and found that it was not distributed normally. When the data is not distributed normally, an intervening large value causes the mean to deviate. For this, I took the median value. But I didn't do this classically by taking the median of a variable. I did it like this; I found the median values ​​of classes 0 and 1 and by taking the average of them, I reached a healthier result. This is the average I get; it created the threshold value that initially demarcated my data. So he separated the 0 and 1 classes. But the threshold process is not over yet. Lets continue :)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding the threshold value according to the age variable\ndf.groupby(\"Exited\").agg({\"Age\":\"median\"}).values.mean()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding the threshold value according to the balance variable\ndf.groupby(\"Exited\").agg({\"Balance\":\"median\"}).values.mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding threshold value according to EstimatedSalary variable\ndf.groupby(\"Exited\").agg({\"EstimatedSalary\":\"median\"}).values.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The 3 cells above formed our first threshold value. We could fill in according to these 3 thresholds and get a low classification success. But here we have to pay attention to this; there is an imbalance between classes in the data. For this, we will use the quartiles and use the values ​​obtained as threshold values. So we will do a classification process by using two threshold values ​​together. Towards class 1 due to the imbalance in the data; So we will get the threshold value from quartiles upwards. You can examine the finishing results by increasing or decreasing the cartridge percentages.\n\nÖrnek :\n\n- First threshold  = df.loc[df.Age > 40.5 ,\"Age\"]\n- Second threshold = df.loc[df.Age > 40.5 ,\"Age\"].quantile(0.90)\n\nClassification achievements corresponding to various quartile values:\n\n- Classification success when 20: 70.49\n- Classification success when 25: 71.54\n- Classification success at 30: 72.86\n- Classification success at 50: 76\n- Classification success at 60: 77\n- Classification success at 80: 78.83\n- Classification success at 90: 79.01"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Finding the quartiles as threshold values.\ndf.loc[df.Age > 40.5 ,\"Age\"].quantile(0.90) \ndf.loc[df.Balance > 100710.98499999999 ,\"Balance\"].quantile(0.90) \ndf.loc[df.EstimatedSalary > 101052.94 ,\"EstimatedSalary\"].quantile(0.90)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We created a new variable by taking a copy of df.\nxx = df.copy()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Determination of classes with threshold values ​​obtained using quartile above. Since we use the \"&\" operator below, classes have been written by applying all the possibilities of the \"&\" operator so that some variables do not appear as empty classes."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# We're opening an empty column. Because we will perform the classification process with reference to the threshold values.\nxx.Exited = np.NaN\n\nxx.loc[(xx.Age > 92) & (xx.Balance > 250898.09) & (xx.EstimatedSalary > 199992.48) , \"Exited\" ] = 1\nxx.loc[(xx.Age > 92.0) & (xx.Balance > 250898.09) , \"Exited\"] = 1\nxx.loc[(xx.Age > 92.0) & (xx.EstimatedSalary > 199992.48) , \"Exited\"] = 1\nxx.loc[(xx.Balance > 250898.09) & (xx.EstimatedSalary > 199992.48) , \"Exited\"] = 1\n\nxx.loc[(xx.Age <= 92.0) & (xx.Balance <= 250898.09) & (xx.EstimatedSalary <= 199992.48) , \"Exited\"] = 0\nxx.loc[(xx.Age <= 92.0) & (xx.Balance <= 250898.09) , \"Exited\"] = 0\nxx.loc[(xx.Age <= 92.0) & (xx.EstimatedSalary <= 199992.48) , \"Exited\"] = 0\nxx.loc[(xx.Balance <= 250898.09) & (xx.EstimatedSalary <= 199992.48) , \"Exited\"] = 0\n\n# Assigned class values ​​come in the form of float.\n#For the score function, we need to convert it to a categorical variable.\n\nxx.Exited =  xx.Exited.astype('category')\nxx.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Searching for missing values\nxx.isnull().sum()\n\n#Total number of observations in the data set\nlen(xx)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Comparison of the target variable created using threshold values ​​and the actual target variable y_test\nconfusion_matrix(xx.Exited, df.Exited)\naccuracy_score(xx.Exited, df.Exited)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"So, classification can be done manually too :)\n\nSee you in another article.\n\nlinkedin : https://www.linkedin.com/in/mehmet-kele%C5%9F-531032174/"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}